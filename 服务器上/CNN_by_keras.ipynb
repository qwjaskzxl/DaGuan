{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 498683 word vectors,\n",
      "Found 102277 texts. 19 labels\n"
     ]
    }
   ],
   "source": [
    "# 借鉴\n",
    "# https://www.jianshu.com/p/7eed068ff353\n",
    "# https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "BASE_DIR = '/home/stu19/perl5/'\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'Golve_keras/glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'Golve_keras/20_newsgroup')\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "MAX_NUM_WORDS = 10000\n",
    "EMBEDDING_DIM = 200\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "embeddings_index = {}\n",
    "w2v_model = Word2Vec.load('./word2vec/w2v_200_mincount3.model')\n",
    "for word in w2v_model.wv.vocab:\n",
    "    coefs = np.asarray(w2v_model.wv[word], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "print('Found %s word vectors,' % len(embeddings_index))\n",
    "\n",
    "texts = []  # list of text samples 每个都是一个str\n",
    "labels = []  # list of label ids\n",
    "labels_num = 0\n",
    "with open('/home/stu19/perl5/myself/train_set.csv', encoding = 'utf-8') as f1:\n",
    "    for line in f1.readlines()[1:]:\n",
    "        line = line.split(',')\n",
    "        if str(int(line[3])-1) not in labels:\n",
    "            labels_num += 1\n",
    "        texts.append(line[2])\n",
    "        labels.append(str(int(line[3])-1))\n",
    "print(\"Found %s texts.\" % len(texts), \"%s labels\" % labels_num)\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(labels[1])\n",
    "labels = to_categorical(np.asarray(labels)) # label one hot，这里由于label是从1开始的。。。所以他会多出来一个0类型\n",
    "print(labels[1])\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "print(data.shape, x_train.shape, y_train.shape, x_val.shape, y_val.shape)\n",
    "print(x_train[1])\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False) #so as to keep the embeddings fixed\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(labels_num, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "# model.add(Dropout(0.2, input_shape=(60,)))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=256,\n",
    "          epochs=100,\n",
    "          validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
