{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumption on loading: 2.872018814086914\n",
      "(99999, 2768408)\n",
      "Time consumption on training1: 384.3682017326355\n",
      "Accuracy: 0.79 (+/- 0.01)\n",
      "Time consumption on training1+2: 1408.990906715393\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         1\n",
      "       0.98      0.97      0.97      5262\n",
      "        10\n",
      "       0.99      0.99      0.99      4846\n",
      "        11\n",
      "       0.98      0.99      0.99      3500\n",
      "        12\n",
      "       0.99      0.99      0.99      5212\n",
      "        13\n",
      "       0.98      0.99      0.99      7712\n",
      "        14\n",
      "       0.99      0.99      0.99      6601\n",
      "        15\n",
      "       1.00      1.00      1.00      7322\n",
      "        16\n",
      "       0.98      0.98      0.98      3147\n",
      "        17\n",
      "       0.98      0.99      0.98      3021\n",
      "        18\n",
      "       0.99      1.00      1.00      6902\n",
      "        19\n",
      "       0.98      0.98      0.98      5427\n",
      "         2\n",
      "       0.99      0.99      0.99      2840\n",
      "         3\n",
      "       0.99      1.00      1.00      8143\n",
      "         4\n",
      "       0.99      0.99      0.99      3734\n",
      "         5\n",
      "       0.99      0.98      0.99      2317\n",
      "         6\n",
      "       1.00      1.00      1.00      6738\n",
      "         7\n",
      "       0.99      0.98      0.99      2973\n",
      "         8\n",
      "       0.98      0.98      0.98      6809\n",
      "         9\n",
      "       1.00      1.00      1.00      7493\n",
      "\n",
      "avg / total       0.99      0.99      0.99     99999\n",
      "\n",
      "(2277, 2768408)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         1\n",
      "       0.64      0.56      0.60       113\n",
      "        10\n",
      "       0.72      0.78      0.75       117\n",
      "        11\n",
      "       0.65      0.69      0.67        71\n",
      "        12\n",
      "       0.73      0.72      0.72       114\n",
      "        13\n",
      "       0.74      0.79      0.76       194\n",
      "        14\n",
      "       0.77      0.81      0.79       139\n",
      "        15\n",
      "       0.92      0.90      0.91       189\n",
      "        16\n",
      "       0.80      0.67      0.73        73\n",
      "        17\n",
      "       0.81      0.74      0.77        73\n",
      "        18\n",
      "       0.91      0.90      0.91       164\n",
      "        19\n",
      "       0.65      0.67      0.66        97\n",
      "         2\n",
      "       0.78      0.82      0.80        61\n",
      "         3\n",
      "       0.92      0.85      0.88       170\n",
      "         4\n",
      "       0.89      0.88      0.88        90\n",
      "         5\n",
      "       0.90      0.88      0.89        52\n",
      "         6\n",
      "       0.90      0.91      0.91       150\n",
      "         7\n",
      "       0.83      0.66      0.74        65\n",
      "         8\n",
      "       0.70      0.79      0.74       163\n",
      "         9\n",
      "       0.91      0.92      0.92       182\n",
      "\n",
      "avg / total       0.81      0.81      0.81      2277\n",
      "\n",
      "Time consumption on testing: 7.378719091415405\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer #计算tfidf\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #计算df\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  #“一步到位”\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals import joblib # to save\n",
    "from sklearn import metrics\n",
    "\n",
    "id_num = []\n",
    "word_article = []\n",
    "words_article = []\n",
    "label = []\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "load_start = time.time()\n",
    "importlib.reload(sys)\n",
    "with open('train_set.csv','r',encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()[1:]\n",
    "    random.shuffle(lines) #注意用法\n",
    "    for line in lines[1:100000]:\n",
    "        line = line.split(',')\n",
    "        X_train.append(line[2])\n",
    "        y_train.append(line[3])\n",
    "    for line in lines[100000:]: #划分出测试集\n",
    "        line = line.split(',')\n",
    "        X_test.append(line[2])\n",
    "        y_test.append(line[3])\n",
    "print(\"Time consumption on loading:\",time.time() - load_start)\n",
    "\n",
    "train_start = time.time()\n",
    "vect = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9, norm='l2', use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "vect.fit(X_train) # 或者直接X_train = vect.fit_transform(X_train)\n",
    "X_train = vect.transform(X_train)\n",
    "print(X_train.shape)\n",
    "# X_train = preprocessing.normalize(X_train, norm='l2', axis=1, copy=True) #变化之后是...一样的\n",
    "# ss=StandardScaler(with_mean=False).fit(X_train)\n",
    "# X_train=ss.transform(X_train)\n",
    "# X_train = preprocessing.scale(X_train, with_mean=False) #标准化\n",
    "# X_train = X_train.toarray() #由稀疏矩阵变成ndarray类型的完整矩阵。。？\n",
    "# pca = PCA(n_components = 10000)\n",
    "# print(X_train.shape)\n",
    "# X_train = pca.fit_transform(X_train)\n",
    "# print(X_train.shape)\n",
    "print(\"Time consumption on training1:\",time.time() - train_start)\n",
    "\n",
    "clf = svm.LinearSVC(C=0.8, loss='squared_hinge', intercept_scaling=1, tol=0.0002, multi_class='ovr')\n",
    "# clf = svm.SVC(kernel='linear', C=1, )\n",
    "# clf = SGDClassifier(loss=\"modified_huber\", )#penalty=”elasticnet”: L2和L1的convex组合; (1 - l1_ratio) * L2 + l1_ratio * L1\n",
    "# parameters = {#'loss':('squared_hinge', 'hinge'),\n",
    "#               #'alpha':[0.00004, 0.00001, 0.00002],\n",
    "#               #'penalty':['elasticnet','l2']\n",
    "#                 'tot':[0.00005, 0.0001, 0.0002]\n",
    "# }\n",
    "# grid_search = GridSearchCV(clf, parameters, n_jobs=-1, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_parameters = dict(grid_search.best_estimator_.get_params())#get实例中的最优参数\n",
    "# for param_name in sorted(parameters.keys()):\n",
    "#     print(\"\\t%s: %r\" % (param_name, best_parameters[param_name])) #输出参数结果\n",
    "clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(clf, X_train, y_train , cv=5)\n",
    "# print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "# joblib.dump(clf, 'lineSVM_20000__.m')\n",
    "print(\"Time consumption on training1+2:\",time.time() - train_start)\n",
    "\n",
    "\n",
    "test_start = time.time()\n",
    "# clf = joblib.load('lineSVM_20000__.m')\n",
    "prediction = clf.predict(X_train)\n",
    "print(metrics.classification_report(y_train,prediction))\n",
    "X_test = vect.transform(X_test)\n",
    "# X_test=ss.transform(X_test)\n",
    "# X_test = X_test.toarray()\n",
    "# X_test = preprocessing.normalize(X_test, norm='l2', axis=1, copy=True) #正则化\n",
    "# X_test = preprocessing.scale(X_test, with_mean=False) \n",
    "print(X_test.shape)\n",
    "prediction = clf.predict(X_test) \n",
    "print(metrics.classification_report(y_test,prediction))\n",
    "# print(metrics.confusion_matrix(y_test,prediction))\n",
    "print(\"Time consumption on testing:\",time.time() - test_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
