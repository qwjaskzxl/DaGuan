{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumption on loading: 2.016610860824585\n",
      "BESTTTT：\tvect__ngram_range: (1, 2)\n",
      "Time consumption on training: 636.975100517273\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         1\n",
      "       0.99      0.98      0.98       773\n",
      "        10\n",
      "       0.99      0.99      0.99       720\n",
      "        11\n",
      "       0.99      0.99      0.99       539\n",
      "        12\n",
      "       1.00      1.00      1.00       775\n",
      "        13\n",
      "       0.99      1.00      0.99      1216\n",
      "        14\n",
      "       1.00      0.99      0.99       988\n",
      "        15\n",
      "       1.00      1.00      1.00      1122\n",
      "        16\n",
      "       0.99      0.98      0.98       469\n",
      "        17\n",
      "       0.99      0.99      0.99       454\n",
      "        18\n",
      "       1.00      1.00      1.00      1054\n",
      "        19\n",
      "       1.00      0.99      0.99       806\n",
      "         2\n",
      "       0.99      1.00      1.00       408\n",
      "         3\n",
      "       0.99      1.00      1.00      1268\n",
      "         4\n",
      "       1.00      1.00      1.00       538\n",
      "         5\n",
      "       1.00      0.99      1.00       330\n",
      "         6\n",
      "       1.00      1.00      1.00      1034\n",
      "         7\n",
      "       0.99      0.99      0.99       433\n",
      "         8\n",
      "       0.99      0.99      0.99      1005\n",
      "         9\n",
      "       0.99      0.99      0.99      1067\n",
      "\n",
      "avg / total       0.99      0.99      0.99     14999\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         1\n",
      "       0.63      0.57      0.60       118\n",
      "        10\n",
      "       0.69      0.60      0.64       124\n",
      "        11\n",
      "       0.59      0.56      0.58        78\n",
      "        12\n",
      "       0.71      0.65      0.68       142\n",
      "        13\n",
      "       0.63      0.78      0.70       167\n",
      "        14\n",
      "       0.77      0.79      0.78       154\n",
      "        15\n",
      "       0.82      0.83      0.83       153\n",
      "        16\n",
      "       0.48      0.38      0.43        68\n",
      "        17\n",
      "       0.69      0.75      0.72        71\n",
      "        18\n",
      "       0.81      0.88      0.85       139\n",
      "        19\n",
      "       0.56      0.57      0.57       127\n",
      "         2\n",
      "       0.67      0.67      0.67        43\n",
      "         3\n",
      "       0.84      0.85      0.85       200\n",
      "         4\n",
      "       0.79      0.80      0.79        89\n",
      "         5\n",
      "       0.84      0.70      0.76        53\n",
      "         6\n",
      "       0.91      0.85      0.88       151\n",
      "         7\n",
      "       0.79      0.70      0.74        76\n",
      "         8\n",
      "       0.64      0.70      0.67       158\n",
      "         9\n",
      "       0.88      0.86      0.87       167\n",
      "\n",
      "avg / total       0.74      0.74      0.73      2278\n",
      "\n",
      "Time consumption on testing: 11.283740997314453\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer #计算tfidf\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #计算df\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  #“一步到位”\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "\n",
    "id_num = []\n",
    "word_article = []\n",
    "words_article = []\n",
    "label = []\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "load_start = time.time()\n",
    "importlib.reload(sys)\n",
    "with open('train_set.csv','r',encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    random.shuffle(lines)\n",
    "    for line in lines[1:15000]:\n",
    "        line = line.split(',')\n",
    "        X_train.append(line[2])\n",
    "        y_train.append(line[3])\n",
    "    for line in lines[100000:]:\n",
    "        line = line.split(',')\n",
    "        X_test.append(line[2])\n",
    "        y_test.append(line[3])\n",
    "print(\"Time consumption on loading:\",time.time() - load_start)\n",
    "\n",
    "\n",
    "train_start = time.time()\n",
    "step = [\n",
    "            ('vect',CountVectorizer(max_df=0.9, min_df=3)),\n",
    "            ('tfidf',TfidfTransformer()),\n",
    "#             ('pca',PCA(n_components=3000)),\n",
    "#             ('clf',RandomForestClassifier()),\n",
    "#             ('clf',svm.SVC(C=1,kernel='linear'))\n",
    "            ('clf',svm.LinearSVC(C=1))\n",
    "#             ('clf',SGDClassifier(loss=\"modified_huber\", penalty=\"elasticnet\", alpha=0.00001, n_jobs=-1))\n",
    "#             ('clf',tree.DecisionTreeClassifier())\n",
    "        ]\n",
    "ppl_clf = Pipeline(step)\n",
    "parameters = {\n",
    "    'vect__ngram_range': [ (1, 2), (1, 3)], #'vect__max_df': (0.9, 0.95),'vect__min_df': (3, 10),\n",
    "#     'tfidf__use_idf': (True, False),#'tfidf__norm': ('l1', 'l2'),\n",
    "#     'clf__alpha': (0.00001, 0.000001), 'clf__n_iter': (10, 50) \n",
    "}\n",
    "\n",
    "#GridSearchCV用于寻找vectorizer词频统计, tfidftransformer特征变换和分类模型的最优参数\n",
    "grid_search = GridSearchCV(ppl_clf, parameters,)\n",
    "grid_search.fit(X_train,y_train) #遍历执行候选参数，寻找最优参数\n",
    "best_parameters = dict(grid_search.best_estimator_.get_params())#get实例中的最优参数\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"BESTTTT：\\t%s: %r\" % (param_name, best_parameters[param_name])),'\\n'#输出参数结果\n",
    "\n",
    "ppl_clf.fit(X_train, y_train) #将ppl_clf实例中的参数重写为最优结果\n",
    "# joblib.dump(ppl_clf, 'lineSVM_20000__.m')\n",
    "print(\"Time consumption on training:\",time.time() - train_start)\n",
    "\n",
    "\n",
    "test_start = time.time()\n",
    "prediction = ppl_clf.predict(X_train) #训练集上准确率\n",
    "print(metrics.classification_report(y_train,prediction))\n",
    "prediction = ppl_clf.predict(X_test) #测试集上准确率\n",
    "print(metrics.classification_report(y_test,prediction))\n",
    "print(\"Time consumption on testing:\",time.time() - test_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
