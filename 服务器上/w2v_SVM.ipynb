{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumption on loading: 2.778332233428955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amax/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:67: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (79999, 256)\n",
      "Time consumption on feature_training: 594.886815071106\n",
      "Time consumption on clf_training: 758.9142446517944\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         1\n",
      "       0.78      0.70      0.74      4232\n",
      "        10\n",
      "       0.82      0.81      0.81      3829\n",
      "        11\n",
      "       0.76      0.80      0.78      2748\n",
      "        12\n",
      "       0.81      0.80      0.81      4157\n",
      "        13\n",
      "       0.82      0.87      0.85      6159\n",
      "        14\n",
      "       0.81      0.88      0.85      5257\n",
      "        15\n",
      "       0.95      0.93      0.94      5862\n",
      "        16\n",
      "       0.86      0.72      0.79      2539\n",
      "        17\n",
      "       0.83      0.83      0.83      2447\n",
      "        18\n",
      "       0.92      0.92      0.92      5554\n",
      "        19\n",
      "       0.70      0.75      0.73      4363\n",
      "         2\n",
      "       0.88      0.88      0.88      2266\n",
      "         3\n",
      "       0.92      0.91      0.91      6502\n",
      "         4\n",
      "       0.91      0.91      0.91      2964\n",
      "         5\n",
      "       0.94      0.87      0.90      1857\n",
      "         6\n",
      "       0.97      0.93      0.95      5377\n",
      "         7\n",
      "       0.82      0.74      0.78      2393\n",
      "         8\n",
      "       0.82      0.86      0.84      5479\n",
      "         9\n",
      "       0.94      0.95      0.94      6014\n",
      "\n",
      "avg / total       0.86      0.86      0.86     79999\n",
      "\n",
      "shape: (22277, 256)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         1\n",
      "       0.56      0.54      0.55      1143\n",
      "        10\n",
      "       0.69      0.69      0.69      1133\n",
      "        11\n",
      "       0.64      0.69      0.66       823\n",
      "        12\n",
      "       0.65      0.64      0.64      1169\n",
      "        13\n",
      "       0.72      0.77      0.74      1748\n",
      "        14\n",
      "       0.74      0.80      0.77      1483\n",
      "        15\n",
      "       0.91      0.88      0.89      1649\n",
      "        16\n",
      "       0.65      0.50      0.57       681\n",
      "        17\n",
      "       0.71      0.73      0.72       647\n",
      "        18\n",
      "       0.88      0.87      0.87      1512\n",
      "        19\n",
      "       0.55      0.61      0.58      1161\n",
      "         2\n",
      "       0.76      0.74      0.75       635\n",
      "         3\n",
      "       0.86      0.84      0.85      1811\n",
      "         4\n",
      "       0.86      0.82      0.84       860\n",
      "         5\n",
      "       0.85      0.75      0.79       512\n",
      "         6\n",
      "       0.92      0.88      0.90      1511\n",
      "         7\n",
      "       0.73      0.66      0.69       645\n",
      "         8\n",
      "       0.69      0.74      0.71      1493\n",
      "         9\n",
      "       0.91      0.92      0.91      1661\n",
      "\n",
      "avg / total       0.77      0.76      0.76     22277\n",
      "\n",
      "Time consumption on testing: 1366.318691253662\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer #计算tfidf\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #计算df\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  #“一步到位”\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "import gensim.models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "\n",
    "id_num = []\n",
    "word_article = []\n",
    "words_article = []\n",
    "label = []\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "load_start = time.time()\n",
    "importlib.reload(sys)\n",
    "with open('train_set.csv','r',encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()[1:]\n",
    "    random.shuffle(lines)\n",
    "    for line in lines[1:20000]:\n",
    "        line = line.split(',')\n",
    "        X_train.append(line[2])\n",
    "        y_train.append(line[3])\n",
    "    for line in lines[950000:]:\n",
    "        line = line.split(',')\n",
    "        X_test.append(line[2])\n",
    "        y_test.append(line[3])\n",
    "print(\"Time consumption on loading:\",time.time() - load_start)\n",
    "\n",
    "train_start = time.time()\n",
    "w2v_size = 256\n",
    "w2v_model = Word2Vec.load('./word2vec/w2v_by_fullset.model')\n",
    "def tran_X(X_list):\n",
    "    X = [[] for i in range(len(X_list))]\n",
    "    for i in range(0,len(X_list)):\n",
    "        j = X_list[i].split(' ') #把str分成词list\n",
    "        X[i] += j \n",
    "#     print(X)\n",
    "    X_vectors = np.zeros((len(X),w2v_size))\n",
    "    i = 0\n",
    "    for words in X:\n",
    "        j = 0\n",
    "        for word in words:\n",
    "            if word in w2v_model.wv.vocab:\n",
    "                X_vectors[i] += w2v_model[word] \n",
    "                j += 1\n",
    "        X_vectors[i] /= j\n",
    "        i += 1\n",
    "    print(\"shape:\",X_vectors.shape)\n",
    "    \n",
    "    imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    X_vectors = imp.fit_transform(X_vectors)\n",
    "#     for i in range(len(X_vectors)): \n",
    "#         for j in range(len(X_vectors[i])):\n",
    "#             if np.isnan(X_vectors[i][j]): #去除NaN\n",
    "#                 X_vectors[i][j] = 0\n",
    "    \n",
    "    X_vectors = preprocessing.normalize(X_vectors, norm='l2', axis=1, copy=True) #正则化 74 68/73 70\n",
    "#     ss=StandardScaler() \n",
    "#     X_vectors=ss.fit_transform(X_vectors) #70 67/70 68\n",
    "#     X_vectors = preprocessing.scale(X_vectors) #标准化 69 67/70 66\n",
    "#     X_vectors = preprocessing.MinMaxScaler().fit_transform(X_vectors) #归一化 71 65/72 65\n",
    "#     model_vectors = np.transpose(model_vectors) #转置\n",
    "    return X_vectors\n",
    "\n",
    "X_train = tran_X(X_train)\n",
    "print(\"Time consumption on feature_training:\",time.time() - train_start)\n",
    "train_start = time.time()\n",
    "# clf = svm.LinearSVC(C=10) #C=1/10：72 73已经是最好水平\n",
    "clf = svm.SVC(kernel='rbf', C=4, gamma=2) #C=4 gamma=2：77 76已经是最好水平\n",
    "clf = svm.SVC(kernel='rbf', C=4, gamma=2)\n",
    "# clf = SGDClassifier(loss=\"modified_huber\", penalty=\"elasticnet\", alpha=0.0001, n_jobs=-1)# modified_huber;  penalty=\"elasticnet\"不然l1 l2recall上不去？\n",
    "# clf = SGDClassifier(loss=\"log\", penalty=\"elasticnet\", alpha=0.000005, n_jobs=-1)\n",
    "# parameters = {#'loss':('log', 'hinge','modified_huber'),\n",
    "#               #'alpha':[0.000002, 0.00001, 0.0001],\n",
    "#               #'penalty':['elasticnet','l2']\n",
    "#                 'C':[1,2,4,8],\n",
    "#                 'gamma':[1,2,4,8]\n",
    "#             }\n",
    "# grid_search = GridSearchCV(clf, parameters, n_jobs=-1, cv=3)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# best_parameters = dict(grid_search.best_estimator_.get_params())#get实例中的最优参数\n",
    "# for param_name in sorted(parameters.keys()):\n",
    "#     print(\"\\t%s: %r\" % (param_name, best_parameters[param_name])) #输出参数结果\n",
    "clf.fit(X_train, y_train)\n",
    "# joblib.dump(clf, 'lineSVM_20000__.m')\n",
    "print(\"Time consumption on clf_training:\",time.time() - train_start)\n",
    "\n",
    "test_start = time.time()\n",
    "# clf = joblib.load('lineSVM_20000__.m')\n",
    "prediction = clf.predict(X_train) #训练集上准确率\n",
    "print(metrics.classification_report(y_train,prediction))\n",
    "X_test = tran_X(X_test)\n",
    "prediction = clf.predict(X_test) #测试集上准确率\n",
    "print(metrics.classification_report(y_test,prediction))\n",
    "# print(metrics.confusion_matrix(y_test,prediction))\n",
    "print(\"Time consumption on testing:\",time.time() - test_start)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
