{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumption on loading: 4.979656457901001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amax/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:84: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time consumption on training: 869.7680995464325\n",
      "Time consumption on testing: 253.66622519493103\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer #计算tfidf\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #计算df\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  #“一步到位”\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "\n",
    "id_num = []\n",
    "word_article = []\n",
    "words_article = []\n",
    "label = []\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "load_start = time.time()\n",
    "importlib.reload(sys)\n",
    "with open('train_set.csv','r',encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:\n",
    "        line = line.split(',')\n",
    "        X_train.append(line[2])\n",
    "        y_train.append(line[3])\n",
    "with open('test_set.csv','r',encoding = 'utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:\n",
    "        line = line.split(',')\n",
    "        X_test.append(line[2])\n",
    "print(\"Time consumption on loading:\",time.time() - load_start)\n",
    "\"\"\"\n",
    "step = [\n",
    "            ('vect',CountVectorizer()),\n",
    "            ('tfidf',TfidfTransformer()),\n",
    "#             ('pca',PCA(n_components=3000)),\n",
    "#             ('clf',RandomForestClassifier()),\n",
    "#             ('clf',svm.SVC(C=1,kernel='linear'))\n",
    "            ('clf',svm.LinearSVC(C=0.3))\n",
    "#             ('clf',tree.DecisionTreeClassifier())\n",
    "        ]\n",
    "ppl_clf = Pipeline(step)\n",
    "\"\"\"\n",
    "train_start = time.time()\n",
    "vect = TfidfVectorizer(ngram_range=(1,3), min_df=3, max_df=0.9, use_idf=1, smooth_idf=1, sublinear_tf=1)\n",
    "vect.fit(X_train) # 或者直接X_train = vect.fit_transform(X_train)\n",
    "X_train = vect.transform(X_train)\n",
    "# ppl_clf = svm.LinearSVC(C=1)\n",
    "# ppl_clf = svm.SVC(kernel='linear', C=0.8, )\n",
    "ppl_clf = SGDClassifier(loss=\"modified_huber\", penalty=\"elasticnet\", alpha=0.00001, n_jobs=-1)\n",
    "ppl_clf.fit(X_train, y_train)\n",
    "# joblib.dump(ppl_clf, 'lineSVM_20000__.m')\n",
    "del X_train,y_train #释放内存\n",
    "print(\"Time consumption on training:\",time.time() - train_start)\n",
    "\n",
    "test_start = time.time()\n",
    "X_test = vect.transform(X_test)\n",
    "# ppl_clf = joblib.load('lineSVM_20000__.m')\n",
    "prediction = ppl_clf.predict(X_test)\n",
    "f_out = open('ans.csv','w')\n",
    "f_out.write(\"id,class\"+\"\\n\")\n",
    "for i in range(X_test.shape[0]): #是列数，即句子数\n",
    "    f_out.write(str(i)+\",\"+str(prediction[i]))\n",
    "f_out.close()\n",
    "# print(metrics.classification_report(y_test,prediction))\n",
    "# print(metrics.confusion_matrix(y_test,prediction))\n",
    "print(\"Time consumption on testing:\",time.time() - test_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
